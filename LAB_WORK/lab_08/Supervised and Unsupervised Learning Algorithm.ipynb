{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQ+yDaWDh4LAz+Ps0owz10"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"r8az93ACiguD"},"outputs":[],"source":["#Linear Regression....regression\n","from sklearn.linear_model import LinearRegression\n","import numpy as np\n","\n","X = np.array([[1500], [1800], [2200], [2500]])\n","y = np.array([300000, 350000, 400000, 450000])\n","\n","model = LinearRegression()\n","model.fit(X, y)\n","\n","new_area = np.array([[2000]])\n","predicted_price = model.predict(new_area)\n","print(\"Predicted Price:\", predicted_price)\n"]},{"cell_type":"code","source":["#Polynomial Regression\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import LinearRegression\n","import numpy as np\n","\n","X = np.array([[1], [2], [3], [4]])\n","y = np.array([1, 4, 9, 16])\n","\n","poly_features = PolynomialFeatures(degree=2)\n","X_poly = poly_features.fit_transform(X)\n","\n","model_poly = LinearRegression()\n","model_poly.fit(X_poly, y)\n","\n","X_new = np.array([[5]])\n","X_new_poly = poly_features.transform(X_new)\n","predicted_value = model_poly.predict(X_new_poly)\n","\n","print(\"Predicted Value (Polynomial Regression):\", predicted_value)\n","\n"],"metadata":{"id":"Wxy9oAbsi9az"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Support Vector Machine (SVM) Regression\n","from sklearn.svm import SVR\n","import numpy as np\n","\n","\n","X = np.array([[1], [2], [3], [4]])\n","y = np.array([2, 3, 4, 5])\n","\n","svm_regressor = SVR(kernel='linear')\n","svm_regressor.fit(X, y)\n","\n","X_new = np.array([[5]])\n","predicted_value = svm_regressor.predict(X_new)\n","\n","print(\"Predicted Value (SVM Regression):\", predicted_value)\n"],"metadata":{"id":"SwdmShaQjKKd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Decision Tree Regression\n","from sklearn.tree import DecisionTreeRegressor\n","import numpy as np\n","\n","X = np.array([[1], [2], [3], [4]])\n","y = np.array([2, 3, 4, 5])\n","\n","tree_regressor = DecisionTreeRegressor()\n","tree_regressor.fit(X, y)\n","\n","X_new = np.array([[5]])\n","predicted_value = tree_regressor.predict(X_new)\n","\n","print(\"Predicted Value (Decision Tree Regression):\", predicted_value)\n","# Output: [6.]\n"],"metadata":{"id":"dIPhBR9_jUXo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Random Forest Regression\n","from sklearn.ensemble import RandomForestRegressor\n","import numpy as np\n","\n","X = np.array([[1], [2], [3], [4]])\n","y = np.array([2, 3, 4, 5])\n","\n","forest_regressor = RandomForestRegressor()\n","forest_regressor.fit(X, y)\n","\n","X_new = np.array([[5]])\n","predicted_value = forest_regressor.predict(X_new)\n","\n","print(\"Predicted Value (Random Forest Regression):\", predicted_value)\n","\n"],"metadata":{"id":"sjuh3yrojfwT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Logistic Regression.....classification\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","model = LogisticRegression()\n","model.fit(X_train, y_train)\n","\n","y_pred = model.predict(X_test)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy (Logistic Regression):\", accuracy)"],"metadata":{"id":"-DlrlXHPkeFT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Support Vector Machines (SVM)\n","from sklearn.svm import SVC\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","\n","model = SVC(kernel='linear')\n","model.fit(X_train, y_train)\n","\n","\n","y_pred = model.predict(X_test)\n","\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy (SVM):\", accuracy)\n"],"metadata":{"id":"ndzSoSxwlJBx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Decision Trees\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","\n","model = DecisionTreeClassifier()\n","model.fit(X_train, y_train)\n","\n","\n","y_pred = model.predict(X_test)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy (Decision Tree):\", accuracy)\n"],"metadata":{"id":"0Qy4PkV1lW6W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Random Forests\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","model = RandomForestClassifier()\n","model.fit(X_train, y_train)\n","\n","y_pred = model.predict(X_test)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy (Random Forest):\", accuracy)\n"],"metadata":{"id":"vSCYMKeIlgh4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Naive Bayes\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","model = GaussianNB()\n","model.fit(X_train, y_train)\n","\n","y_pred = model.predict(X_test)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy (Naive Bayes):\", accuracy)"],"metadata":{"id":"EblH-hrel5Za"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Hierarchical Clustering\n","from sklearn.cluster import AgglomerativeClustering\n","from sklearn.datasets import make_blobs\n","import matplotlib.pyplot as plt\n","# Generate synthetic data\n","X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60,random_state=0)\n","# Perform Hierarchical Clustering\n","agg_clustering = AgglomerativeClustering(n_clusters=4)\n","labels = agg_clustering.fit_predict(X)\n","# Visualize the clusters\n","plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n","plt.title(\"Hierarchical Clustering\")\n","plt.show()"],"metadata":{"id":"geZ3GBTbmC0p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#K-means Clustering\n","from sklearn.cluster import KMeans\n","from sklearn.datasets import make_blobs\n","import matplotlib.pyplot as plt\n","# Generate synthetic data\n","X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n","# Perform K-means Clustering\n","kmeans = KMeans(n_clusters=4)\n","labels = kmeans.fit_predict(X)\n","# Visualize the clusters\n","plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n","plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200,\n","c='red', marker='X', label='Centroids')\n","plt.title(\"K-means Clustering\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"tGqPxu42mEUq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Principal Component Analysis (PCA) clusetering\n","from sklearn.decomposition import PCA\n","from sklearn.datasets import load_iris\n","import matplotlib.pyplot as plt\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","# Perform PCA for visualization (2D projection)\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X)\n","# Visualize PCA-transformed data\n","plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n","plt.title(\"PCA Visualization of Iris Dataset\")\n","plt.xlabel(\"Principal Component 1\")\n","plt.ylabel(\"Principal Component 2\")\n","plt.show()"],"metadata":{"id":"gNNTYKqYmHZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Gaussian Mixture Models (GMMs) clustering\n","from sklearn.mixture import GaussianMixture\n","from sklearn.datasets import make_blobs\n","import matplotlib.pyplot as plt\n","# Generate synthetic data\n","X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n","# Perform Gaussian Mixture Model (GMM) clustering\n","gmm = GaussianMixture(n_components=4)\n","labels = gmm.fit_predict(X)\n","# Visualize the clusters\n","plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n","plt.title(\"Gaussian Mixture Model (GMM) Clustering\")\n","plt.show()"],"metadata":{"id":"SReVlgmJmLsf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Density-Based Spatial Clustering of Applications with Noise (DBSCAN) clustering\n","from sklearn.cluster import DBSCAN\n","from sklearn.datasets import make_blobs\n","import matplotlib.pyplot as plt\n","# Generate synthetic data\n","X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n","# Perform DBSCAN clustering\n","dbscan = DBSCAN(eps=0.5, min_samples=5)\n","labels = dbscan.fit_predict(X)\n","# Visualize the clusters (including noise points)\n","plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n","plt.title(\"DBSCAN Clustering\")\n","plt.show()"],"metadata":{"id":"Pb7beA8imMx4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Association rule learning...unsupervised Apriori Algorithm\n","from mlxtend.frequent_patterns import apriori\n","from mlxtend.preprocessing import TransactionEncoder\n","import pandas as pd\n","# Sample transaction dataset (list of lists)\n","dataset = [['bread', 'milk', 'eggs'],\n","['bread', 'butter'],\n","['milk', 'butter'],\n","['bread', 'milk', 'butter'],\n","['bread', 'milk']]\n","# One-hot encode the dataset\n","te = TransactionEncoder()\n","te_ary = te.fit(dataset).transform(dataset)\n","df = pd.DataFrame(te_ary, columns=te.columns_)\n","# Apply Apriori algorithm to find frequent itemsets\n","frequent_itemsets = apriori(df, min_support=0.4, use_colnames=True)\n","print(frequent_itemsets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7bNIXdQ5mWrZ","executionInfo":{"status":"ok","timestamp":1715195261059,"user_tz":-300,"elapsed":2772,"user":{"displayName":"k224120 Izza Farhat","userId":"10701324734490980400"}},"outputId":"a09e47b0-2804-40fd-848d-660988d2575a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   support         itemsets\n","0      0.8          (bread)\n","1      0.6         (butter)\n","2      0.8           (milk)\n","3      0.4  (bread, butter)\n","4      0.6    (milk, bread)\n","5      0.4   (milk, butter)\n"]}]},{"cell_type":"code","source":["#Association rule learning...unsupervised FP-Growth Algorithm\n","from mlxtend.frequent_patterns import fpgrowth\n","from mlxtend.preprocessing import TransactionEncoder\n","import pandas as pd\n","# Sample transaction dataset (list of lists)\n","dataset = [['bread', 'milk', 'eggs'],\n","['bread', 'butter'],\n","['milk', 'butter'],\n","['bread', 'milk', 'butter'],\n","['bread', 'milk']]\n","# One-hot encode the dataset\n","te = TransactionEncoder()\n","te_ary = te.fit(dataset).transform(dataset)\n","df = pd.DataFrame(te_ary, columns=te.columns_)\n","# Apply FP-Growth algorithm to find frequent itemsets\n","frequent_itemsets = fpgrowth(df, min_support=0.4, use_colnames=True)\n","print(frequent_itemsets)"],"metadata":{"id":"-S3h60PfmZkR"},"execution_count":null,"outputs":[]}]}