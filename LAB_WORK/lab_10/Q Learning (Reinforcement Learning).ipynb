{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQowd8k9yIsLXkUQSjH21c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Icmlp7pRshsX"},"outputs":[],"source":["import numpy as np\n","import gym\n","\n","env = gym.make('FrozenLake-v1')\n","\n","alpha = 0.1\n","gamma = 0.99\n","num_episodes = 5000\n","\n","\n","num_states = env.observation_space.n\n","num_actions = env.action_space.n\n","Q = np.zeros((num_states, num_actions))Q = np.zeros((env.observation_space.n, env.action_space.n))\n","\n","for i in range(num_episodes):\n","\n","state = env.reset()\n","done = False\n","total_reward = 0\n","\n","while not done:\n","\n","if np.random.uniform(0, 1) < 0.5:\n","action = env.action_space.sample()\n","else:\n","action = np.argmax(Q[state, :])\n","\n","\n","next_state, reward, done, info = env.step(action)\n","\n","Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])\n","\n","state = next_state\n","total_reward += reward\n","\n","print(f\"Episode {i}: Totalreward = {total_reward}\")\n","\n","num_test_episodes = 100\n","num_test_steps = 100\n","num_successes = 0\n","\n","for i in range(num_test_episodes):\n","state = env.reset()\n","done = False\n","steps = 0\n","\n","while not done and steps < num_test_steps:\n","\n","action = np.argmax(Q[state, :])\n","\n","next_state, reward, done, info = env.step(action)\n","\n","\n","state = next_state\n","steps += 1\n","\n","ifstate == 15:\n","num_successes += 1\n","\n","print(\"Successrate:\", num_successes/num_test_episodes)"]},{"cell_type":"code","source":["!pip install numpy\n","!pip install gym\n","import numpy as np\n","import gym\n","import random"],"metadata":{"id":"X7haAG-6tTvh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = gym.make(\"FrozenLake-v1\")\n","action_size = env.action_space.n\n","state_size = env.observation_space.n\n","\n","qtable = np.zeros((state_size, action_size))\n","print(qtable)\n","total_episodes = 20000\n","learning_rate = 0.7\n","max_steps = 99\n","gamma = 0.7\n","\n","\n","epsilon = 1.0\n","max_epsilon = 1.0\n","min_epsilon = 0.01\n","decay_rate = 0.5\n","\n","rewards = []\n","\n","for episode in range(total_episodes):\n","    state = env.reset()\n","    step = 0\n","    done = False\n","    total_rewards = 0\n","\n","    for step in range(max_steps):\n","        exp_exp_tradeoff = random.uniform(0, 1)\n","\n","        if exp_exp_tradeoff > epsilon:\n","            action = np.argmax(qtable[state,:])\n","\n","        else:\n","            action = env.ample()action_space.s\n","\n","        new_state, reward, done, info = env.step(action)\n","\n","        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n","        total_rewards += reward\n","\n","        state = new_state\n","\n","        if done == True:\n","            break\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n","    rewards.append(total_rewards)\n","\n","\n","print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n","print(qtable)\n","env.reset()\n","\n","for episode in range(10):\n","    state = env.reset()\n","    step = 0\n","    done = False\n","    print(\"****************************************************\")\n","    print(\"EPISODE \", episode)\n","\n","    for step in range(max_steps):\n","\n","        action = np.argmax(qtable[state,:])\n","\n","        new_state, reward, done, info = env.step(action)\n","\n","        if done:\n","\n","            if new_state == 15:\n","                print(\"We reached our Goal üèÜ\")\n","            else:\n","                print(\"We fell into a hole ‚ò†Ô∏è\")\n","\n","            print(\"Number of steps\", step)\n","\n","            break\n","        state = new_state\n","env.close()"],"metadata":{"id":"U0stBxICuAfE"},"execution_count":null,"outputs":[]}]}